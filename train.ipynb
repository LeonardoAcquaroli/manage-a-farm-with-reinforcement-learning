{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import os\n",
    "import source.config as config\n",
    "from source.algorithms import *\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import source.farm_env # Import necessary to register the gym environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(id='FarmEnv-v0',\n",
    "                initial_budget = config.INITIAL_BUDGET,\n",
    "                sheep_cost = config.SHEEP_COST,\n",
    "                wheat_cost = config.WHEAT_COST,\n",
    "                wool_price = config.WOOL_PRICE,\n",
    "                wheat_price = config.WHEAT_PRICE,\n",
    "                max_years = config.MAX_YEARS,\n",
    "                wool_fixed_cost = config.WOOL_FIXED_COST,\n",
    "                storm_probability = config.STORM_PROBABILITY,\n",
    "                incest_penalty = config.INCEST_PENALTY,\n",
    "                reward_std = config.SIGMA\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(a, win=100):\n",
    "    return np.convolve(a, np.ones(win), mode='same') / win"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "policy_learning_rate = 1e-4\n",
    "value_learning_rate = 1e-4\n",
    "n_episodes = 500_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "REINFORCE_agent = FarmAgentNeuralREINFORCEAdvantage(\n",
    "    environment=env, policy_learning_rate=policy_learning_rate, value_learning_rate=value_learning_rate, epsilon=start_epsilon, epsilon_decay=epsilon_decay, final_epsilon=final_epsilon, gamma=.999,\n",
    "    # policy_net_weights_path='agent_models/policy_net_weights.pth', value_net_weights_path='agent_models/value_net_weights.pth'\n",
    ")\n",
    "\n",
    "MC_vfa_agent = FarmAgentMCVFA(\n",
    "    environment=env, learning_rate=learning_rate, epsilon=start_epsilon, epsilon_decay=epsilon_decay, final_epsilon=final_epsilon, gamma=.999\n",
    ")\n",
    "\n",
    "SARSA_vfa_agent = FarmAgentSarsaVFA(\n",
    "    environment=env, learning_rate=learning_rate, epsilon=start_epsilon, epsilon_decay=epsilon_decay, final_epsilon=final_epsilon, gamma=.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nenv = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "final_budget_queue = []\n",
    "\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    REINFORCE_agent.update(episode_number=episode)\n",
    "    REINFORCE_agent.decay_epsilon()\n",
    "    final_budget_queue.append(nenv.unwrapped.budget)\n",
    "\n",
    "    # Optionally, you can print some statistics every N episodes\n",
    "    if episode % 10000 == 0:\n",
    "        avg_budget = sum(final_budget_queue[-10000:]) / min(10000, len(final_budget_queue))\n",
    "        print(f\"Episode {episode}, Average Budget: {avg_budget:.2f}\") #, Weights: {list(REINFORCE_agent.policy_net.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1000\n",
    "# rewards = conv(np.array(nenv.return_queue).flatten(), win=w)\n",
    "# lengths = conv(np.array(nenv.length_queue).flatten(), win=w)\n",
    "# error = conv(np.array(REINFORCE_agent.training_error).flatten(), win=w)\n",
    "final_budgets = conv(np.array(final_budget_queue), win=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4), ncols=2)\n",
    "sns.lineplot(x=range(len(np.array(0).flatten())), y=np.array(0).flatten(), ax=ax[0], c='#acc700', linewidth=.2)\n",
    "ax[0].set_title('Error')\n",
    "sns.lineplot(x=range(len(final_budget_queue)), y=final_budget_queue, ax=ax[1], c='#acc700', linewidth=.2)\n",
    "ax[1].set_title('Final budget')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "sns.lineplot(x=range(len(final_budgets)), y=final_budgets, ax=ax, c='#acc700', linewidth=.2)\n",
    "ax.set_title('Final budget')\n",
    "\n",
    "# Remove spines on top and right sides\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "for _ in range(30):\n",
    "    options = env.unwrapped.actions_available\n",
    "    action = REINFORCE_agent.policy(state, greedy=True)\n",
    "    \n",
    "    s_prime, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f'Action: {action}', f'State: {state}', f'Reward: {round(reward,3)}', f'Terminated: {terminated}', truncated, info)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f'Final state: {s_prime}')\n",
    "        print(\"============End of episode============\")\n",
    "        state, info = env.reset()\n",
    "        break\n",
    "    else:\n",
    "        state = s_prime\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder = 's6-penalty2.5-edecay1-g0.999-storm0.6'\n",
    "os.makedirs(f'agent_models/REINFORCENeuralAdvantage/{experiment_folder}', exist_ok=True)\n",
    "torch.save(REINFORCE_agent.policy_net.state_dict(), f'agent_models/REINFORCENeuralAdvantage/{experiment_folder}/policy_net_weights-{int(n_episodes/1000)}k.pth')\n",
    "torch.save(REINFORCE_agent.value_net.state_dict(), f'agent_models/REINFORCENeuralAdvantage/{experiment_folder}/value_net_weights-{int(n_episodes/1000)}k.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'data/REINFORCENeuralAdvantage/{experiment_folder}', exist_ok=True)\n",
    "np.save(f'data/REINFORCENeuralAdvantage/{experiment_folder}/final_budget_queue-{int(n_episodes/1000)}k.npy', np.array(final_budget_queue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montecarlo VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nenv = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "final_budget_queue = []\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    MC_vfa_agent.update(episode_number=episode)\n",
    "    MC_vfa_agent.decay_epsilon()\n",
    "    final_budget_queue.append(nenv.unwrapped.budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 1000\n",
    "# rewards = conv(np.array(nenv.return_queue).flatten(), win=w)\n",
    "# lengths = conv(np.array(nenv.length_queue).flatten(), win=w)\n",
    "error = conv(np.array(MC_vfa_agent.training_error).flatten(), win=w)\n",
    "final_budgets = conv(np.array(final_budget_queue), win=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(error), len(final_budgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4), ncols=2)\n",
    "sns.lineplot(x=range(len(np.array(MC_vfa_agent.training_error).flatten())), y=np.array(MC_vfa_agent.training_error).flatten(), ax=ax[0], c='#acc700', linewidth=.2)\n",
    "ax[0].set_title('Error')\n",
    "sns.lineplot(x=range(len(final_budget_queue)), y=final_budget_queue, ax=ax[1], c='#acc700', linewidth=.2)\n",
    "ax[1].set_title('Final budget')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 4), ncols=2)\n",
    "sns.lineplot(x=range(len(error)), y=error, ax=ax[0], c='#acc700', linewidth=.2)\n",
    "ax[0].set_title('Error')\n",
    "sns.lineplot(x=range(len(final_budgets)), y=final_budgets, ax=ax[1], c='#acc700', linewidth=.2)\n",
    "ax[1].set_title('Final budget')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MC_vfa_agent.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, info = env.reset()\n",
    "for _ in range(30):\n",
    "    options = env.unwrapped.actions_available\n",
    "    action = MC_vfa_agent.greedy_policy(state)\n",
    "    \n",
    "    s_prime, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f'Action: {action}', f'State: {state}', f'Reward: {round(reward,3)}', f'Terminated: {terminated}', truncated, info)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f'Final state: {s_prime}')\n",
    "        print(\"============End of episode============\")\n",
    "        state, info = env.reset()\n",
    "        break\n",
    "    else:\n",
    "        state = s_prime\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_folder = 'gaussian_delta-nowoolcost-15woolprice-200k'\n",
    "os.makedirs(f'data/MCVFA/{experiment_folder}', exist_ok=True)\n",
    "\n",
    "np.save(f'data/MCVFA/{experiment_folder}/weights.npy', MC_vfa_agent.w.detach().numpy())\n",
    "np.save(f'data/MCVFA/{experiment_folder}/final_budget_queue.npy', np.array(final_budget_queue))\n",
    "np.save(f'data/MCVFA/{experiment_folder}/training_error.npy', np.array(MC_vfa_agent.training_error).flatten())\n",
    "# Split the 15M training error records in two chunks not to exceed GitHub limit of 100MB per file\n",
    "# np.save('data/MCVFA/training_error_0k-750k.npy', MC_vfa_agent.training_error[: len(MC_vfa_agent.training_error) // 2])\n",
    "# np.save('data/MCVFA/training_error_750k-1.5M.npy', MC_vfa_agent.training_error[len(MC_vfa_agent.training_error) // 2 : ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nenv = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
    "final_budget_queue = []\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    state, info = nenv.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = SARSA_vfa_agent.policy(state)\n",
    "        # print(action)\n",
    "        s_prime, reward, terminated, truncated, info = nenv.step(action=action)\n",
    "        # update\n",
    "        SARSA_vfa_agent.update(state, action, reward, s_prime)\n",
    "        done = terminated or truncated\n",
    "        state = s_prime\n",
    "    SARSA_vfa_agent.decay_epsilon()\n",
    "    final_budget_queue.append(nenv.unwrapped.budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 100\n",
    "rewards = conv(np.array(nenv.return_queue).flatten(), win=w)\n",
    "lengths = conv(np.array(nenv.length_queue).flatten(), win=w)\n",
    "error = conv(np.array(SARSA_vfa_agent.training_error).flatten(), win=w)\n",
    "final_budgets = conv(np.array(final_budget_queue), win=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rewards),len(lengths),len(SARSA_vfa_agent.training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 4), ncols=4)\n",
    "ax[0].plot(range(len(nenv.return_queue)), nenv.return_queue, c='#acc700', linewidth=.4)\n",
    "ax[0].set_title('Reward (Average)')\n",
    "ax[1].plot(range(len(nenv.length_queue)), nenv.length_queue, c='#acc700', linewidth=.8)\n",
    "ax[1].set_title('Episode Length')\n",
    "ax[2].plot(range(len(SARSA_vfa_agent.training_error)), SARSA_vfa_agent.training_error, c='#acc700', linewidth=.2)\n",
    "ax[2].set_title('Error')\n",
    "ax[3].plot(range(len(final_budget_queue)), final_budget_queue, c='#acc700', linewidth=.2)\n",
    "ax[3].set_title('Final budget')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 4), ncols=4)\n",
    "sns.lineplot(x=range(len(rewards)), y=rewards, ax=ax[0], c='#acc700', linewidth=.4)\n",
    "ax[0].set_title('Reward (Average)')\n",
    "sns.lineplot(x=range(len(lengths)), y=lengths, ax=ax[1], c='#acc700', linewidth=.4)\n",
    "ax[1].set_title('Episode Length')\n",
    "sns.lineplot(x=range(len(error)), y=error, ax=ax[2], c='#acc700', linewidth=.2)\n",
    "ax[2].set_title('Error')\n",
    "sns.lineplot(x=range(len(final_budgets)), y=final_budgets, ax=ax[3], c='#acc700', linewidth=.2)\n",
    "ax[3].set_title('Final budget')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SARSA_vfa_agent.w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manage-a-farm-mRYmmniZ-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
